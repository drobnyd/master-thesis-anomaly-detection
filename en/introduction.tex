
\chapter{Introduction}
\label{introduction}

Software systems and internet are nowadays omnipresent.

Even in an old-fashioned field like communication over push-to-talk devices, that used to be carried out only through analogue networks can be today complemented by software that detects poor coverage and eventually switches to digital connection. 

This is what Motorola SmartConnect system has been developed for. Since this software system serves people in critical job positions such as policemen, policewomen or firefighters, it is crucial that SmartConnect's infrastructure is as robust as possible.

If something goes wrong, it is desirable to firstly recognize that a problem happened and, ideally, to be able to target the problem where it happened in order to act upon it.

For observing and troubleshooting software system's state, it is a good practice for developers to introduce logs.
However, in a complex system like Motorola SmartConnect is, comprised of many different services which produce gigabytes of raw textual data per day in logs, it is becoming unfeasible to track down and figure out solutions to issues by humans.

Not only that there is way too much information in terms of size, logs from different services may be located at different places and it is unrealistic to expect from a person to uncover the internal relationships between various services which lead to errors.

The goal of our thesis is to investigate, whether a reliable solution that could automate the troubleshooting process could be developed using anomaly detection techniques. The solution should be of help for developers and customer support working in the SmartConnect team.

Looking at the logs produced by the telecommunication system as time series data should allow us to experiment with machine learning models detecting scenarios that include erroneous runs of the system.

After proving that this is possible, we aim to invent a tool that would be able to carry out this heavy cumbersome work automatically, monitor Motorola's production system and raise an alarm when an anomaly is detected, or, even better, predicted.

From there a responsible person can pick up the issue with narrowed scope and investigate further on the alleged issue.

In the thesis, we aim to define steps that are necessary to proceed in order to deliver such solution. We also show what possible strategies and tools can be deployed in each of the steps and reason which possibilities are the optimal for this use case.


\section{Research Questions}
% In this paper, we explore anomalies in log data and existing anomaly detection techniques [3].
% He: To bridge this gap, in this paper, we provide a detailed review and evaluation of log-based anomaly detection, as well as release an open-source toolkit1 for anomaly detection. Our goal is not to improve any specific method, but to portray an overall picture of current research on log analysis for anomaly detection. We believe that our work can benefit researchers and practitioners in two aspects: The review can help them grasp a quick understanding of current anomaly detection methods; while the open-source toolkit allows them to easily reuse existing methods and make further customization or improvement. This helps avoid time-consuming yet redundant efforts for re-implementation.

% check log_anomaly-new paper as inspiration
With our master thesis, we are trying to answer the following research questions: 

Firstly, we conducted review of literature on research that has been previously done in the field of anomaly detection on log data. There has been good amount of work done before us, therefore we are able to evaluate which methods are used on problems that are similar to ours and what is their advantages and disadvantages. Based on that we pick some methods that seem to be well suited and we argue why we it makes sense to apply them on our data.\\

% Maybe something more general, like can anomaly detection be applied on Motorola SmartConnect log data? Applying on real-world data. Are their logs informative enough for anomaly detection? 

% Then order the questions in the same way as our anomaly detection workflow. First Log abbstraction and feature extraction -> RQ3 first. 

% Then the anomaly detection method themselves.
    
    \textit{RQ1: Which anomaly detection techniques are applicable on time series data produced by Motorola SmartConnect system in unsupervised manner?}\\
    
After that, we proceed with selected techniques and conduct series of experiments.\\

    \textit{RQ2: How are the techniques performing on the given log dataset and why they do or do not they yield satisfying results? If not, what could be improved about the log that would increase the performance of anomaly detection?}\\ 
    
    \textit{RQ3: Does a weighted event vector representation of raw logged messages logged by Motorola Solution's SmartConnect production system provide sufficient information for anomaly detection?}
    
\section{Glossary}

\glsaddall
\printglossaries


\section{Outline}

\section{Related Work}
Log-based anomaly detection has been widely studied. Anomaly detection research usually follows similar steps. At first, log parser is used to extract log templates from unstructured log data. Then the logs are transformed into a numerical feature vector using the log templates. Lastly, anomaly detection techniques are applied. A crucial assumption is that event types are extracted correctly. Various approaches to each of these steps were presented in previous work on this topic. 

Xu et al. \cite{xu2009} was one of the first ones to apply Principal Component Analysis (PCA) to achieve anomaly detection in log data. They generated event templates based on the source code and used event count matrix as an input to PCA. To form an input, Xu uses the concept of windowing by sessions, where all the logs sharing the same session ID are grouped together. Event count matrix is constructed such that each vector in a row represents events in one session ID and each column vector is an event type. The dataset used for their experiments were collected from the Hadoop Distributed File System (HDFS).
Similar approach was employed by He, \cite{he2016}, who also used event count matrix. In order to extract features, they groups log data into three groups: fixed windows, sliding windows, and session windows. 

Invariant Mining (IM) method was proposed by Lou et al. \cite{lou2010} while also using log event count matrix as an input. IM mines the invariants among log events from log event count vectors. Event count vectors that do not satisfy mined invariants are considered anomalies. 

Recently, as deep learning outperforms the traditional machine learning as the scale of data increases \cite{Sydney2019DeepLF}, neural networks have been also applied to the problem of anomaly detection in system logs. 

Du et al. \cite{duLSTM2017} used deep neural network (DNN) model composed of Long Short-Term Memory (LSTM) units to model logs as natural language sequence. The model is trained using non-anomalous log data. This way, LSTM can automatically learn normal behaviour, forecast the next event type and identify anomalies if it differs from the actual event type. Similarly, Zhang et al. \cite{zhang2016} and DeepLog \cite{deeplog2017} also use LSTM deep learning approach. The former borrowed an idea from term frequency-inverse document frequency (TF-IDF) by taking all the logs in each time window as a document and use TF-IDF weight as a feature representation. The latter work differs from other DNN approaches, as it retains timestamp when encoding log message and performs anomaly detection for each log entry, rather than for each session.
