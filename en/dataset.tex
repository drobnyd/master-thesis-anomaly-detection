\chapter{Dataset}

The primary dataset contains log traces gathered from multiple services comprising Motorola SmartConnect's Cloud Infrastructure Engineering development environment.\\
The services producing the logs are ran within Kubernetes Pods \cite{K8s:pods}. In order to obtain the logs, we came up with a pipeline that periodically connects to the pods\todo[]{depends on where we are getting logs from - not settled yet}, gathers and preprocesses them into such form that the data can be fed to selected machine learning algorithms or plotted. More on the preprocessing step can be found in Chapter \ref{methodology}. \\
In this chapter, we pay attention to what important properties of the logs can be retrieved from the raw raw data in the first place.
Kubernetes records 16 properties per single log entry\todo[]{depends on where we are getting logs from - not settled yet}.\\
Timestamps are in the format \texttt{YYYY-MM-DDTHH:mm:SS.sssZ} - therefore year (YYYY) is a four digit zero padded number, month (\texttt{MM}), day (\texttt{DD}), hour (\texttt{HH}), minutes (\texttt{mm}) and seconds (\texttt{SS}) are two digit zero padded numbers and milliseconds(\texttt{ssss}) is a three digit zero padded number. The time part is separated from date by a single \texttt{T} character and the whole timestamp ends with a letter \texttt{Z}.\todo{also depends on where we get the logs from}\\

The dataset contains no labeling of the logs, however, we strove to obtain solely anomaly free data from the expected behavior of the system.\\
At night, the system undergoes tests. During the testing, normal behavior of the telecommunication system is mimicked - all the required set up of infrastructure takes place and simulations of calls between groups of push-to-talk radios are happening. This way, the system produces logs that our machine learning models are compatible with.
In case the nightly tests are all passed, it should be sound that the collected data are either completely free of anomalies, respectively contain only very small percentage of them therefore we train our models on vastly anomaly free dataset. This statement strongly relies on the assumption that the functionality of the system is well covered by the tests. 
Furthermore, we assume that if a test succeeds, the execution of the test represents an anomaly free scenario.

We gathered the data throughout \todo[]{n} nightly test runs between dates \todo[]{date-start - date-end}.


\section{Data Characteristics}
\todo{add the amount of data we have in numbers}
Out of the 16 log properties, we decided to look only at \texttt{msg} and \texttt{timestamp} as the others (such as kubernetes pod name, process id of the service generating the log, etc. all off the properties are listed in Appendix \todo{appendix with log properties}) or are too specific, such as \texttt{pod}. 
Including fewer predictor variables should in general avoid us overfitting and make the final model more transparent.
Therefore, in order to produce as general solution as possible, we stick only to those two predictors.\\

The data that the system under test is producing, however slightly differ from the live production environment.
The main difference is that the scenarios that the test suit generates are serialized meaning that maximum one call is taking place at each moment.
On the other hand, the production environment is not limiting the number broadcasts at a time. Therefore, the logs of the production environment may include much higher counts of some events. \todo[]{plots showing events in serialized calls vs overlapping ones}\\
\todo[]{do we do something about the serialization like trying to modify timestamps in the data to "parallelize" the calls?}

\section{Types of Anomalies}
In this section, we present a couple of known anomalies as a motivation for what anomaly scenarios we aim to target with our solution.
Also, we are going to benchmark our approaches and models against these anomalies, however the objective is to be also able to detect anomalies that we do not have prior knowledge of. 
In other words, we aim to identify even anomalous scenarios that are spotted for the first time in the production environment and do not appear in our dataset.


\subsection{Database Outage}
The first anomaly that we describe is not something that we would expect happen too often in the production system. 
However, it is an easy use case to observe through collected logs. 
In this anomaly, a database that is orchestrating the call logic is taken down and radios that are in a call at that moment bonk as they're unable to communicate with each other. 
In response to that, the database recovers itself in no more than 2 seconds mostly. After that broadcast should continue.

\todo{Figure with plots comparing anomaly and normal behavior}

\subsection{Weakening Connection}
A user with radio moves and his connection to the internet is getting worse - he is getting away from his source of internet connection. This should eventually lead to an increased loss of data packets delivered to the user. For him, this ultimately results in receiving distorted audio from other members of the broadcast.

\todo{Figure with plots comparing anomaly and normal behavior}

\todo{Out of date credentials scenario}
